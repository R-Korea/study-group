---
title: "Lab 4.6"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 4.6.1 The Stock Market Data

* examining some numerical and graphical summaries of the `Smarket` data

```{r}
library(ISLR)
names(Smarket)
```

```{r}
dim(Smarket)
```

```{r}
summary(Smarket)
```

```{r}
pairs(Smarket)
```

```{r}
cor(Smarket[,-9])
```

```{r}
attach(Smarket)
plot(Volume)
```


## 4.6.2 Logistic Regression

* `glm()`: generalized linear models, a class of models that includes logistic regression

```{r}
glm.fits = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial)
# family=binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.

summary(glm.fits)
```

```{r}
coef(glm.fits) # to access just the coefficients for the model
```
```{r}
summary(glm.fits)$coef
```
```{r}
summary(glm.fits)$coef[,4] # Pr(>|z|) column only
```

* predict

```{r}
glm.probs = predict(glm.fits, type="response")
glm.probs[1:10]
```

```{r}
contrasts(Direction)
```

* create a vector of class predictions

```{r}
glm.pred = rep("Down", 1250) # creates a vector of 1250 Down elements
glm.pred[glm.probs > .5] = "Up" # transforms to Up all of the elements which the predicted probbability of a market increase exceeds 0.5.
```

```{r}
table(glm.pred, Direction) # confusion matrix
```

```{r}
mean(glm.pred==Direction) 
# = (507+145)/1250 : 52.16%, correctly predicted by logistic regression
# However, this result is misleading because we trained and tested the model on the same set of 1250 observations.
```

* let's fit the model using part of the data and predict the held out data

```{r}
train = (Year<2005)
Smarket.2005 = Smarket[!train,]
dim(Smarket.2005)
```
```{r}
Direction.2005 = Direction[!train]
```

* fit a logistic regression model

```{r}
glm.fits = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
               data = Smarket,
               family = binomial, subset = train)
glm.probs = predict(glm.fits, Smarket.2005, type="response")
```


* compare the predictions to the actual movements of the market

```{r}
glm.pred = rep("Down", 252)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2005)
```

```{r}
mean(glm.pred==Direction.2005)

# test set error rate: around 52%
mean(glm.pred!=Direction.2005)
```

* refit the logistic regression using just `Lag1` and `Lag2` which seemed to have the highest predictie power in the original logistic regression model

```{r}
glm.fits = glm(Direction~Lag1+Lag2, 
               data = Smarket,
               family = binomial, subset = train)
glm.probs = predict(glm.fits, Smarket.2005, type = "response")

glm.pred = rep("Down", 252)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2005)
```

```{r}
# accuracy: tp + tn / tp + tn + fn + fp
mean(glm.pred==Direction.2005)

# precision: tp / tp + fp
106/(106+76)
```

* predict `Direction`

```{r}
predict(glm.fits, 
        newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
        type = "response")
```




## 4.6.3 Linear Discriminat Analysis

* fit an LDA model on the `Smarket` data

```{r}
library(MASS)
lda.fit = lda(Direction~Lag1+Lag2,
              data = Smarket, subset = train)
lda.fit
```

* prediction

```{r}
lda.pred = predict(lda.fit, Smarket.2005)
names(lda.pred)
```

```{r}
lda.class = lda.pred$class
table(lda.class, Direction.2005)
```

```{r}
mean(lda.class == Direction.2005)
```

* apply a 50% threshold to the posterior probabilities

```{r}
# count 'Down'
sum(lda.pred$posterior[,1] >= .5)
sum(lda.pred$posterior[,1] < .5)
```

```{r}
lda.pred$posterior[1:20, 1]
lda.class[1:20]
```

* apply a 90% threshold to the posterior probabilities

```{r}
sum(lda.pred$posterior[,1] >.9)
```


## 4.6.4 Quadratic Discriminatn Analysis

* fit a QDA model to the `Smarket` data

```{r}
qda.fit = qda(Direction~Lag1+Lag2,
              data = Smarket, subset = train)
qda.fit
```

```{r}
qda.class = predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
```

```{r}
# accuracy
mean(qda.class == Direction.2005)

# precision
81/(81+121)
```


## 4.6.5 K-Nearest Neighbors

* prepare the dataset

```{r}
library(class)
train.X = cbind(Lag1, Lag2)[train,]
test.X = cbind(Lag1, Lag2)[!train,]
train.Direction = Direction[train]
```

* predict using KNN (k=1, overly flexible)

```{r}
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k=1)
table(knn.pred, Direction.2005)
```

```{r}
# accuracy
mean(knn.pred == Direction.2005)
```

* predict using KNN (k=3)

```{r}
knn.pred = knn(train.X, test.X, train.Direction, k=3)
table(knn.pred, Direction.2005)
```

```{r}
mean(knn.pred == Direction.2005)
```


## 4.6.6 An Application to Caravan Insurance Data

* `Caravan` dataset

```{r}
dim(Caravan)
```

```{r}
attach(Caravan)
summary(Purchase)
```

```{r}
# only 6% of people purchased caravan insurance
348/5822
```

* standardize the data (`scale()` function)

```{r}
standardized.X = scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
```

```{r}
var(standardized.X[,1])
var(standardized.X[,2])
```

* split the data and predict using KNN

```{r}
test = 1:1000
train.X = standardized.X[-test,]
test.X = standardized.X[test,]

train.Y = Purchase[-test]
test.Y = Purchase[test]

set.seed(1)
knn.pred = knn(train.X, test.X, train.Y, k=1)
```

```{r}
# error rate
mean(test.Y != knn.pred)
```

```{r}
table(knn.pred, test.Y)
```

```{r}
# precision
9/(68+9)
```

* KNN when k: precision=19% 

```{r}
knn.pred = knn(train.X, test.X, train.Y, k=3)
table(knn.pred, test.Y)
```
```{r}
# precision
5/26
```

* KNN when k=5: precision=26.7%

```{r}
knn.pred = knn(train.X, test.X, train.Y, k=5)
table(knn.pred, test.Y)
```
```{r}
# precision
4/15
```

* compare with a logistic regression model to the data

```{r}
glm.fits = glm(Purchase~., 
               data = Caravan, subset = -test,
               family = binomial)
```

```{r}
glm.probs = predict(glm.fits, Caravan[test,], type="response")

# threshold = .5
glm.pred = rep("No", 1000)
glm.pred[glm.probs > .5] = "Yes"
table(glm.pred, test.Y)
```

```{r}
# threshold = .25
glm.pred = rep("No", 1000)
glm.pred[glm.probs > .25] = "Yes"
table(glm.pred, test.Y)
```
```{r}
# precision = 33%: over five times better than random guessing
11/(22+11)
```
